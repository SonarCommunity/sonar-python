<p>This rule raises an issue when a machine learning estimator or optimizer is instantiated without specifying the important hyperparameters.</p>
<h2>Why is this an issue?</h2>
<p>When instantiating an estimator or an optimizer, default values for any hyperparameters that are not specified will be used. Relying on the default
values can lead to non-reproducible results across different versions of the library.</p>
<p>Furthermore, the default values might not be the best choice for the specific problem at hand and can lead to suboptimal performance.</p>
<p>Here are the estimators and the parameters considered by this rule :</p>
<table>
  <colgroup>
    <col style="width: 50%;">
    <col style="width: 50%;">
  </colgroup>
  <thead>
    <tr>
      <th>Scikit-learn - Estimator</th>
      <th>Hyperparameters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><p>AdaBoostClassifier</p></td>
      <td><p>learning_rate</p></td>
    </tr>
    <tr>
      <td><p>AdaBoostRegressor</p></td>
      <td><p>learning_rate</p></td>
    </tr>
    <tr>
      <td><p>GradientBoostingClassifier</p></td>
      <td><p>learning_rate</p></td>
    </tr>
    <tr>
      <td><p>GradientBoostingRegressor</p></td>
      <td><p>learning_rate</p></td>
    </tr>
    <tr>
      <td><p>HistGradientBoostingClassifier</p></td>
      <td><p>learning_rate</p></td>
    </tr>
    <tr>
      <td><p>HistGradientBoostingRegressor</p></td>
      <td><p>learning_rate</p></td>
    </tr>
    <tr>
      <td><p>RandomForestClassifier</p></td>
      <td><p>min_samples_leaf, max_features</p></td>
    </tr>
    <tr>
      <td><p>RandomForestRegressor</p></td>
      <td><p>min_samples_leaf, max_features</p></td>
    </tr>
    <tr>
      <td><p>ElasticNet</p></td>
      <td><p>alpha, l1_ratio</p></td>
    </tr>
    <tr>
      <td><p>NearestNeighbors</p></td>
      <td><p>n_neighbors</p></td>
    </tr>
    <tr>
      <td><p>KNeighborsClassifier</p></td>
      <td><p>n_neighbors</p></td>
    </tr>
    <tr>
      <td><p>KNeighborsRegressor</p></td>
      <td><p>n_neighbors</p></td>
    </tr>
    <tr>
      <td><p>NuSVC</p></td>
      <td><p>nu, kernel, gamma</p></td>
    </tr>
    <tr>
      <td><p>NuSVR</p></td>
      <td><p>C, kernel, gamma</p></td>
    </tr>
    <tr>
      <td><p>SVC</p></td>
      <td><p>C, kernel, gamma</p></td>
    </tr>
    <tr>
      <td><p>SVR</p></td>
      <td><p>C, kernel, gamma</p></td>
    </tr>
    <tr>
      <td><p>DecisionTreeClassifier</p></td>
      <td><p>ccp_alpha</p></td>
    </tr>
    <tr>
      <td><p>DecisionTreeRegressor</p></td>
      <td><p>ccp_alpha</p></td>
    </tr>
    <tr>
      <td><p>MLPClassifier</p></td>
      <td><p>hidden_layer_sizes</p></td>
    </tr>
    <tr>
      <td><p>MLPRegressor</p></td>
      <td><p>hidden_layer_sizes</p></td>
    </tr>
    <tr>
      <td><p>PolynomialFeatures</p></td>
      <td><p>degree, interaction_only</p></td>
    </tr>
  </tbody>
</table>
<table>
  <colgroup>
    <col style="width: 50%;">
    <col style="width: 50%;">
  </colgroup>
  <thead>
    <tr>
      <th>PyTorch - Optimizer</th>
      <th>Hyperparameters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><p>Adadelta</p></td>
      <td><p>lr, weight_decay</p></td>
    </tr>
    <tr>
      <td><p>Adagrad</p></td>
      <td><p>lr, weight_decay</p></td>
    </tr>
    <tr>
      <td><p>Adam</p></td>
      <td><p>lr, weight_decay</p></td>
    </tr>
    <tr>
      <td><p>AdamW</p></td>
      <td><p>lr, weight_decay</p></td>
    </tr>
    <tr>
      <td><p>SparseAdam</p></td>
      <td><p>lr</p></td>
    </tr>
    <tr>
      <td><p>Adamax</p></td>
      <td><p>lr, weight_decay</p></td>
    </tr>
    <tr>
      <td><p>ASGD</p></td>
      <td><p>lr, weight_decay</p></td>
    </tr>
    <tr>
      <td><p>LBFGS</p></td>
      <td><p>lr</p></td>
    </tr>
    <tr>
      <td><p>NAdam</p></td>
      <td><p>lr, weight_decay, momentum_decay</p></td>
    </tr>
    <tr>
      <td><p>RAdam</p></td>
      <td><p>lr, weight_decay</p></td>
    </tr>
    <tr>
      <td><p>RMSprop</p></td>
      <td><p>lr, weight_decay, momentum</p></td>
    </tr>
    <tr>
      <td><p>Rprop</p></td>
      <td><p>lr</p></td>
    </tr>
    <tr>
      <td><p>SGD</p></td>
      <td><p>lr, weight_decay, momentum</p></td>
    </tr>
  </tbody>
</table>
<h2>How to fix it in Scikit-Learn</h2>
<p>Specify the hyperparameters when instantiating the estimator.</p>
<h3>Code examples</h3>
<h4>Noncompliant code example</h4>
<pre data-diff-id="1" data-diff-type="noncompliant">
from sklearn.neighbors import KNeighborsClassifier

clf = KNeighborsClassifier() # Noncompliant : n_neighbors is not specified, different values can change the behaviour of the predictor significantly
</pre>
<h4>Compliant solution</h4>
<pre data-diff-id="1" data-diff-type="compliant">
from sklearn.neighbors import KNeighborsClassifier

clf = KNeighborsClassifier( # Compliant
    n_neighbors=5
)
</pre>
<h2>How to fix it in PyTorch</h2>
<p>Specify the hyperparameters when instantiating the optimizer</p>
<h3>Code examples</h3>
<h4>Noncompliant code example</h4>
<pre data-diff-id="2" data-diff-type="noncompliant">
from my_model import model
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr = 0.001) # Noncompliant : weight_decay is not specified, different values can change the behaviour of the optimizer significantly
</pre>
<h4>Compliant solution</h4>
<pre data-diff-id="2" data-diff-type="compliant">
from my_model import model
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr = 0.001, weight_decay = 0.003) # Compliant
</pre>
<h2>Resources</h2>
<h3>Articles &amp; blog posts</h3>
<ul>
  <li> Probst, P., Boulesteix, A. L., &amp; Bischl, B. (2019). Tunability: Importance of Hyperparameters of Machine Learning Algorithms. Journal of
  Machine Learning Research, 20(53), 1-32. </li>
  <li> van Rijn, J. N., &amp; Hutter, F. (2018, July). Hyperparameter importance across datasets. In Proceedings of the 24th ACM SIGKDD International
  Conference on Knowledge Discovery &amp; Data Mining (pp. 2367-2376). </li>
</ul>
<h3>Documentation</h3>
<ul>
  <li> PyTorch Documentation - <a href="https://pytorch.org/docs/stable/optim.html">torch.optim</a> </li>
</ul>
<h3>External coding guidelines</h3>
<ul>
  <li> Code Smells for Machine Learning Applications - <a
  href="https://hynn01.github.io/ml-smells/posts/codesmells/11-hyperparameter-not-explicitly-set/">Hyperparameter not Explicitly Set</a> </li>
</ul>

