
torch.nn.attentionÀ

SDPBackendtorch._C._SDPBackend"	enum.EnumHrA
ERRORtorch._C._SDPBackend.ERROR
builtins.int"builtins.intr?
MATHtorch._C._SDPBackend.MATH
builtins.int"builtins.intrU
FLASH_ATTENTION$torch._C._SDPBackend.FLASH_ATTENTION
builtins.int"builtins.intr]
EFFICIENT_ATTENTION(torch._C._SDPBackend.EFFICIENT_ATTENTION
builtins.int"builtins.intrU
CUDNN_ATTENTION$torch._C._SDPBackend.CUDNN_ATTENTION
builtins.int"builtins.int‡
_raise_kernel_warnings)torch.nn.attention._raise_kernel_warnings"
None*8
params,
torch._C._SDPAParams"torch._C._SDPAParamsY
_get_flash_version%torch.nn.attention._get_flash_version"
builtins.str"builtins.str*s
__path__torch.nn.attention.__path__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*Ž
__annotations__"torch.nn.attention.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*q
__all__torch.nn.attention.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*g
WARN_FOR_UNFUSED_KERNELS+torch.nn.attention.WARN_FOR_UNFUSED_KERNELS
builtins.bool"builtins.bool