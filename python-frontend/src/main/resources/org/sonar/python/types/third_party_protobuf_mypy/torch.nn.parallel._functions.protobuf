
torch.nn.parallel._functions²
	Broadcast&torch.nn.parallel._functions.Broadcast" torch.autograd.function.Function*q
forward.torch.nn.parallel._functions.Broadcast.forward*
ctx*
target_gpus*

inputs0:staticmethodh*h
backward/torch.nn.parallel._functions.Broadcast.backward*
ctx*
grad_outputs0:staticmethodhæ
ReduceAddCoalesced/torch.nn.parallel._functions.ReduceAddCoalesced" torch.autograd.function.Function*‰
forward7torch.nn.parallel._functions.ReduceAddCoalesced.forward*
ctx*
destination*

num_inputs*	
grads0:staticmethodh*q
backward8torch.nn.parallel._functions.ReduceAddCoalesced.backward*
ctx*
grad_outputs0:staticmethodh°
Gather#torch.nn.parallel._functions.Gather" torch.autograd.function.Function*y
forward+torch.nn.parallel._functions.Gather.forward*
ctx*
target_device*
dim*

inputs0:staticmethodh*d
backward,torch.nn.parallel._functions.Gather.backward*
ctx*
grad_output0:staticmethodhÃ
Scatter$torch.nn.parallel._functions.Scatter" torch.autograd.function.Function*ˆ
forward,torch.nn.parallel._functions.Scatter.forward*
ctx*
target_gpus*
chunk_sizes*
dim*	
input0:staticmethodh*e
backward-torch.nn.parallel._functions.Scatter.backward*
ctx*
grad_output0:staticmethodhp
_get_stream(torch.nn.parallel._functions._get_stream"
Any*.
device"
torch._C.device"torch._C.device*˜
__annotations__,torch.nn.parallel._functions.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*…
_streams%torch.nn.parallel._functions._streamsÑ
6Union[builtins.list[Union[torch._C.Stream,None]],None]Š
*builtins.list[Union[torch._C.Stream,None]]M
Union[torch._C.Stream,None]"
torch._C.Stream"torch._C.Stream
None"builtins.list
None