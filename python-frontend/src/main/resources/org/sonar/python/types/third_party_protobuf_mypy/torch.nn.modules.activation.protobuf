
torch.nn.modules.activationÎ
	Threshold%torch.nn.modules.activation.Threshold"torch.nn.modules.module.Module*´
__init__.torch.nn.modules.activation.Threshold.__init__"
None*X
selfN
%torch.nn.modules.activation.Threshold"%torch.nn.modules.activation.Threshold*/
	threshold 
builtins.float"builtins.float*+
value 
builtins.float"builtins.float*-
inplace
builtins.bool"builtins.bool *˘
forward-torch.nn.modules.activation.Threshold.forward",
torch._tensor.Tensor"torch._tensor.Tensor*X
selfN
%torch.nn.modules.activation.Threshold"%torch.nn.modules.activation.Threshold*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*H

extra_repr0torch.nn.modules.activation.Threshold.extra_repr*
selfrê
__constants__3torch.nn.modules.activation.Threshold.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listr^
	threshold/torch.nn.modules.activation.Threshold.threshold 
builtins.float"builtins.floatrV
value+torch.nn.modules.activation.Threshold.value 
builtins.float"builtins.floatrX
inplace-torch.nn.modules.activation.Threshold.inplace
builtins.bool"builtins.boolÉ
ReLU torch.nn.modules.activation.ReLU"torch.nn.modules.module.Module*æ
__init__)torch.nn.modules.activation.ReLU.__init__"
None*N
selfD
 torch.nn.modules.activation.ReLU" torch.nn.modules.activation.ReLU*-
inplace
builtins.bool"builtins.bool *Í
forward(torch.nn.modules.activation.ReLU.forward",
torch._tensor.Tensor"torch._tensor.Tensor*N
selfD
 torch.nn.modules.activation.ReLU" torch.nn.modules.activation.ReLU*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*ß

extra_repr+torch.nn.modules.activation.ReLU.extra_repr"
builtins.str"builtins.str*N
selfD
 torch.nn.modules.activation.ReLU" torch.nn.modules.activation.ReLUrã
__constants__.torch.nn.modules.activation.ReLU.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrS
inplace(torch.nn.modules.activation.ReLU.inplace
builtins.bool"builtins.boolØ
RReLU!torch.nn.modules.activation.RReLU"torch.nn.modules.module.Module*ü
__init__*torch.nn.modules.activation.RReLU.__init__"
None*P
selfF
!torch.nn.modules.activation.RReLU"!torch.nn.modules.activation.RReLU*-
lower 
builtins.float"builtins.float *-
upper 
builtins.float"builtins.float *-
inplace
builtins.bool"builtins.bool *Ì
forward)torch.nn.modules.activation.RReLU.forward",
torch._tensor.Tensor"torch._tensor.Tensor*P
selfF
!torch.nn.modules.activation.RReLU"!torch.nn.modules.activation.RReLU*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*D

extra_repr,torch.nn.modules.activation.RReLU.extra_repr*
selfrå
__constants__/torch.nn.modules.activation.RReLU.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrR
lower'torch.nn.modules.activation.RReLU.lower 
builtins.float"builtins.floatrR
upper'torch.nn.modules.activation.RReLU.upper 
builtins.float"builtins.floatrT
inplace)torch.nn.modules.activation.RReLU.inplace
builtins.bool"builtins.boolâ
Hardtanh$torch.nn.modules.activation.Hardtanh"torch.nn.modules.module.Module*Ê
__init__-torch.nn.modules.activation.Hardtanh.__init__"
None*V
selfL
$torch.nn.modules.activation.Hardtanh"$torch.nn.modules.activation.Hardtanh*/
min_val 
builtins.float"builtins.float */
max_val 
builtins.float"builtins.float *-
inplace
builtins.bool"builtins.bool *[
	min_valueJ
Union[builtins.float,None] 
builtins.float"builtins.float
None *[
	max_valueJ
Union[builtins.float,None] 
builtins.float"builtins.float
None *ˆ
forward,torch.nn.modules.activation.Hardtanh.forward",
torch._tensor.Tensor"torch._tensor.Tensor*V
selfL
$torch.nn.modules.activation.Hardtanh"$torch.nn.modules.activation.Hardtanh*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*≥

extra_repr/torch.nn.modules.activation.Hardtanh.extra_repr"
builtins.str"builtins.str*V
selfL
$torch.nn.modules.activation.Hardtanh"$torch.nn.modules.activation.Hardtanhrè
__constants__2torch.nn.modules.activation.Hardtanh.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrY
min_val,torch.nn.modules.activation.Hardtanh.min_val 
builtins.float"builtins.floatrY
max_val,torch.nn.modules.activation.Hardtanh.max_val 
builtins.float"builtins.floatrW
inplace,torch.nn.modules.activation.Hardtanh.inplace
builtins.bool"builtins.bool¡
ReLU6!torch.nn.modules.activation.ReLU6"$torch.nn.modules.activation.Hardtanh*¡
__init__*torch.nn.modules.activation.ReLU6.__init__"
None*P
selfF
!torch.nn.modules.activation.ReLU6"!torch.nn.modules.activation.ReLU6*-
inplace
builtins.bool"builtins.bool *™

extra_repr,torch.nn.modules.activation.ReLU6.extra_repr"
builtins.str"builtins.str*P
selfF
!torch.nn.modules.activation.ReLU6"!torch.nn.modules.activation.ReLU6ƒ
Sigmoid#torch.nn.modules.activation.Sigmoid"torch.nn.modules.module.Module*Û
forward+torch.nn.modules.activation.Sigmoid.forward",
torch._tensor.Tensor"torch._tensor.Tensor*T
selfJ
#torch.nn.modules.activation.Sigmoid"#torch.nn.modules.activation.Sigmoid*7
input,
torch._tensor.Tensor"torch._tensor.Tensorü
Hardsigmoid'torch.nn.modules.activation.Hardsigmoid"torch.nn.modules.module.Module*”
__init__0torch.nn.modules.activation.Hardsigmoid.__init__"
None*\
selfR
'torch.nn.modules.activation.Hardsigmoid"'torch.nn.modules.activation.Hardsigmoid*-
inplace
builtins.bool"builtins.bool *ˇ
forward/torch.nn.modules.activation.Hardsigmoid.forward",
torch._tensor.Tensor"torch._tensor.Tensor*\
selfR
'torch.nn.modules.activation.Hardsigmoid"'torch.nn.modules.activation.Hardsigmoid*7
input,
torch._tensor.Tensor"torch._tensor.Tensorrí
__constants__5torch.nn.modules.activation.Hardsigmoid.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrZ
inplace/torch.nn.modules.activation.Hardsigmoid.inplace
builtins.bool"builtins.boolµ
Tanh torch.nn.modules.activation.Tanh"torch.nn.modules.module.Module*Í
forward(torch.nn.modules.activation.Tanh.forward",
torch._tensor.Tensor"torch._tensor.Tensor*N
selfD
 torch.nn.modules.activation.Tanh" torch.nn.modules.activation.Tanh*7
input,
torch._tensor.Tensor"torch._tensor.TensorÉ
SiLU torch.nn.modules.activation.SiLU"torch.nn.modules.module.Module*æ
__init__)torch.nn.modules.activation.SiLU.__init__"
None*N
selfD
 torch.nn.modules.activation.SiLU" torch.nn.modules.activation.SiLU*-
inplace
builtins.bool"builtins.bool *Í
forward(torch.nn.modules.activation.SiLU.forward",
torch._tensor.Tensor"torch._tensor.Tensor*N
selfD
 torch.nn.modules.activation.SiLU" torch.nn.modules.activation.SiLU*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*ß

extra_repr+torch.nn.modules.activation.SiLU.extra_repr"
builtins.str"builtins.str*N
selfD
 torch.nn.modules.activation.SiLU" torch.nn.modules.activation.SiLUrã
__constants__.torch.nn.modules.activation.SiLU.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrS
inplace(torch.nn.modules.activation.SiLU.inplace
builtins.bool"builtins.boolÉ
Mish torch.nn.modules.activation.Mish"torch.nn.modules.module.Module*æ
__init__)torch.nn.modules.activation.Mish.__init__"
None*N
selfD
 torch.nn.modules.activation.Mish" torch.nn.modules.activation.Mish*-
inplace
builtins.bool"builtins.bool *Í
forward(torch.nn.modules.activation.Mish.forward",
torch._tensor.Tensor"torch._tensor.Tensor*N
selfD
 torch.nn.modules.activation.Mish" torch.nn.modules.activation.Mish*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*ß

extra_repr+torch.nn.modules.activation.Mish.extra_repr"
builtins.str"builtins.str*N
selfD
 torch.nn.modules.activation.Mish" torch.nn.modules.activation.Mishrã
__constants__.torch.nn.modules.activation.Mish.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrS
inplace(torch.nn.modules.activation.Mish.inplace
builtins.bool"builtins.boolã
	Hardswish%torch.nn.modules.activation.Hardswish"torch.nn.modules.module.Module*Õ
__init__.torch.nn.modules.activation.Hardswish.__init__"
None*X
selfN
%torch.nn.modules.activation.Hardswish"%torch.nn.modules.activation.Hardswish*-
inplace
builtins.bool"builtins.bool *˘
forward-torch.nn.modules.activation.Hardswish.forward",
torch._tensor.Tensor"torch._tensor.Tensor*X
selfN
%torch.nn.modules.activation.Hardswish"%torch.nn.modules.activation.Hardswish*7
input,
torch._tensor.Tensor"torch._tensor.Tensorrê
__constants__3torch.nn.modules.activation.Hardswish.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrX
inplace-torch.nn.modules.activation.Hardswish.inplace
builtins.bool"builtins.bool˜
ELUtorch.nn.modules.activation.ELU"torch.nn.modules.module.Module*Í
__init__(torch.nn.modules.activation.ELU.__init__"
None*L
selfB
torch.nn.modules.activation.ELU"torch.nn.modules.activation.ELU*-
alpha 
builtins.float"builtins.float *-
inplace
builtins.bool"builtins.bool *Á
forward'torch.nn.modules.activation.ELU.forward",
torch._tensor.Tensor"torch._tensor.Tensor*L
selfB
torch.nn.modules.activation.ELU"torch.nn.modules.activation.ELU*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*§

extra_repr*torch.nn.modules.activation.ELU.extra_repr"
builtins.str"builtins.str*L
selfB
torch.nn.modules.activation.ELU"torch.nn.modules.activation.ELUrä
__constants__-torch.nn.modules.activation.ELU.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrP
alpha%torch.nn.modules.activation.ELU.alpha 
builtins.float"builtins.floatrR
inplace'torch.nn.modules.activation.ELU.inplace
builtins.bool"builtins.boolÖ
CELU torch.nn.modules.activation.CELU"torch.nn.modules.module.Module*Ì
__init__)torch.nn.modules.activation.CELU.__init__"
None*N
selfD
 torch.nn.modules.activation.CELU" torch.nn.modules.activation.CELU*-
alpha 
builtins.float"builtins.float *-
inplace
builtins.bool"builtins.bool *Í
forward(torch.nn.modules.activation.CELU.forward",
torch._tensor.Tensor"torch._tensor.Tensor*N
selfD
 torch.nn.modules.activation.CELU" torch.nn.modules.activation.CELU*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*ß

extra_repr+torch.nn.modules.activation.CELU.extra_repr"
builtins.str"builtins.str*N
selfD
 torch.nn.modules.activation.CELU" torch.nn.modules.activation.CELUrã
__constants__.torch.nn.modules.activation.CELU.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrQ
alpha&torch.nn.modules.activation.CELU.alpha 
builtins.float"builtins.floatrS
inplace(torch.nn.modules.activation.CELU.inplace
builtins.bool"builtins.boolÉ
SELU torch.nn.modules.activation.SELU"torch.nn.modules.module.Module*æ
__init__)torch.nn.modules.activation.SELU.__init__"
None*N
selfD
 torch.nn.modules.activation.SELU" torch.nn.modules.activation.SELU*-
inplace
builtins.bool"builtins.bool *Í
forward(torch.nn.modules.activation.SELU.forward",
torch._tensor.Tensor"torch._tensor.Tensor*N
selfD
 torch.nn.modules.activation.SELU" torch.nn.modules.activation.SELU*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*ß

extra_repr+torch.nn.modules.activation.SELU.extra_repr"
builtins.str"builtins.str*N
selfD
 torch.nn.modules.activation.SELU" torch.nn.modules.activation.SELUrã
__constants__.torch.nn.modules.activation.SELU.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrS
inplace(torch.nn.modules.activation.SELU.inplace
builtins.bool"builtins.boolÊ
GLUtorch.nn.modules.activation.GLU"torch.nn.modules.module.Module*µ
__init__(torch.nn.modules.activation.GLU.__init__"
None*L
selfB
torch.nn.modules.activation.GLU"torch.nn.modules.activation.GLU*'
dim
builtins.int"builtins.int *Á
forward'torch.nn.modules.activation.GLU.forward",
torch._tensor.Tensor"torch._tensor.Tensor*L
selfB
torch.nn.modules.activation.GLU"torch.nn.modules.activation.GLU*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*§

extra_repr*torch.nn.modules.activation.GLU.extra_repr"
builtins.str"builtins.str*L
selfB
torch.nn.modules.activation.GLU"torch.nn.modules.activation.GLUrä
__constants__-torch.nn.modules.activation.GLU.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrH
dim#torch.nn.modules.activation.GLU.dim
builtins.int"builtins.intã
GELU torch.nn.modules.activation.GELU"torch.nn.modules.module.Module*¿
__init__)torch.nn.modules.activation.GELU.__init__"
None*N
selfD
 torch.nn.modules.activation.GELU" torch.nn.modules.activation.GELU*/
approximate
builtins.str"builtins.str *Í
forward(torch.nn.modules.activation.GELU.forward",
torch._tensor.Tensor"torch._tensor.Tensor*N
selfD
 torch.nn.modules.activation.GELU" torch.nn.modules.activation.GELU*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*ß

extra_repr+torch.nn.modules.activation.GELU.extra_repr"
builtins.str"builtins.str*N
selfD
 torch.nn.modules.activation.GELU" torch.nn.modules.activation.GELUrã
__constants__.torch.nn.modules.activation.GELU.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrY
approximate,torch.nn.modules.activation.GELU.approximate
builtins.str"builtins.strœ

Hardshrink&torch.nn.modules.activation.Hardshrink"torch.nn.modules.module.Module*–
__init__/torch.nn.modules.activation.Hardshrink.__init__"
None*Z
selfP
&torch.nn.modules.activation.Hardshrink"&torch.nn.modules.activation.Hardshrink*-
lambd 
builtins.float"builtins.float *¸
forward.torch.nn.modules.activation.Hardshrink.forward",
torch._tensor.Tensor"torch._tensor.Tensor*Z
selfP
&torch.nn.modules.activation.Hardshrink"&torch.nn.modules.activation.Hardshrink*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*π

extra_repr1torch.nn.modules.activation.Hardshrink.extra_repr"
builtins.str"builtins.str*Z
selfP
&torch.nn.modules.activation.Hardshrink"&torch.nn.modules.activation.Hardshrinkrë
__constants__4torch.nn.modules.activation.Hardshrink.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrW
lambd,torch.nn.modules.activation.Hardshrink.lambd 
builtins.float"builtins.floatÊ
	LeakyReLU%torch.nn.modules.activation.LeakyReLU"torch.nn.modules.module.Module*Ö
__init__.torch.nn.modules.activation.LeakyReLU.__init__"
None*X
selfN
%torch.nn.modules.activation.LeakyReLU"%torch.nn.modules.activation.LeakyReLU*6
negative_slope 
builtins.float"builtins.float *-
inplace
builtins.bool"builtins.bool *˘
forward-torch.nn.modules.activation.LeakyReLU.forward",
torch._tensor.Tensor"torch._tensor.Tensor*X
selfN
%torch.nn.modules.activation.LeakyReLU"%torch.nn.modules.activation.LeakyReLU*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*∂

extra_repr0torch.nn.modules.activation.LeakyReLU.extra_repr"
builtins.str"builtins.str*X
selfN
%torch.nn.modules.activation.LeakyReLU"%torch.nn.modules.activation.LeakyReLUrê
__constants__3torch.nn.modules.activation.LeakyReLU.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrX
inplace-torch.nn.modules.activation.LeakyReLU.inplace
builtins.bool"builtins.boolrh
negative_slope4torch.nn.modules.activation.LeakyReLU.negative_slope 
builtins.float"builtins.float”

LogSigmoid&torch.nn.modules.activation.LogSigmoid"torch.nn.modules.module.Module*¸
forward.torch.nn.modules.activation.LogSigmoid.forward",
torch._tensor.Tensor"torch._tensor.Tensor*Z
selfP
&torch.nn.modules.activation.LogSigmoid"&torch.nn.modules.activation.LogSigmoid*7
input,
torch._tensor.Tensor"torch._tensor.Tensorƒ
Softplus$torch.nn.modules.activation.Softplus"torch.nn.modules.module.Module*¸
__init__-torch.nn.modules.activation.Softplus.__init__"
None*V
selfL
$torch.nn.modules.activation.Softplus"$torch.nn.modules.activation.Softplus*,
beta 
builtins.float"builtins.float *1
	threshold 
builtins.float"builtins.float *ˆ
forward,torch.nn.modules.activation.Softplus.forward",
torch._tensor.Tensor"torch._tensor.Tensor*V
selfL
$torch.nn.modules.activation.Softplus"$torch.nn.modules.activation.Softplus*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*≥

extra_repr/torch.nn.modules.activation.Softplus.extra_repr"
builtins.str"builtins.str*V
selfL
$torch.nn.modules.activation.Softplus"$torch.nn.modules.activation.Softplusrè
__constants__2torch.nn.modules.activation.Softplus.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrS
beta)torch.nn.modules.activation.Softplus.beta 
builtins.float"builtins.floatr]
	threshold.torch.nn.modules.activation.Softplus.threshold 
builtins.float"builtins.floatœ

Softshrink&torch.nn.modules.activation.Softshrink"torch.nn.modules.module.Module*–
__init__/torch.nn.modules.activation.Softshrink.__init__"
None*Z
selfP
&torch.nn.modules.activation.Softshrink"&torch.nn.modules.activation.Softshrink*-
lambd 
builtins.float"builtins.float *¸
forward.torch.nn.modules.activation.Softshrink.forward",
torch._tensor.Tensor"torch._tensor.Tensor*Z
selfP
&torch.nn.modules.activation.Softshrink"&torch.nn.modules.activation.Softshrink*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*π

extra_repr1torch.nn.modules.activation.Softshrink.extra_repr"
builtins.str"builtins.str*Z
selfP
&torch.nn.modules.activation.Softshrink"&torch.nn.modules.activation.Softshrinkrë
__constants__4torch.nn.modules.activation.Softshrink.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrW
lambd,torch.nn.modules.activation.Softshrink.lambd 
builtins.float"builtins.floatä#
MultiheadAttention.torch.nn.modules.activation.MultiheadAttention"torch.nn.modules.module.Module*√
__init__7torch.nn.modules.activation.MultiheadAttention.__init__"
None*j
self`
.torch.nn.modules.activation.MultiheadAttention".torch.nn.modules.activation.MultiheadAttention*
	embed_dim
Any*
	num_heads
Any*
dropout
Any *
bias
Any *
add_bias_kv
Any *
add_zero_attn
Any *
kdim
Any *
vdim
Any *
batch_first
Any *
device
Any *
dtype
Any *_
_reset_parameters@torch.nn.modules.activation.MultiheadAttention._reset_parameters*
self*`
__setstate__;torch.nn.modules.activation.MultiheadAttention.__setstate__*
self*	
state*´
forward6torch.nn.modules.activation.MultiheadAttention.forward"Ã
<Tuple[torch._tensor.Tensor,Union[torch._tensor.Tensor,None]],
torch._tensor.Tensor"torch._tensor.Tensor\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None*j
self`
.torch.nn.modules.activation.MultiheadAttention".torch.nn.modules.activation.MultiheadAttention*7
query,
torch._tensor.Tensor"torch._tensor.Tensor*5
key,
torch._tensor.Tensor"torch._tensor.Tensor*7
value,
torch._tensor.Tensor"torch._tensor.Tensor*t
key_padding_mask\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None *2
need_weights
builtins.bool"builtins.bool *m
	attn_mask\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None *:
average_attn_weights
builtins.bool"builtins.bool */
	is_causal
builtins.bool"builtins.bool *∫
merge_masks:torch.nn.modules.activation.MultiheadAttention.merge_masks"Ë
@Tuple[Union[torch._tensor.Tensor,None],Union[builtins.int,None]]\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
None*j
self`
.torch.nn.modules.activation.MultiheadAttention".torch.nn.modules.activation.MultiheadAttention*k
	attn_mask\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None*r
key_padding_mask\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None*7
query,
torch._tensor.Tensor"torch._tensor.Tensorrô
__constants__<torch.nn.modules.activation.MultiheadAttention.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrù
bias_k5torch.nn.modules.activation.MultiheadAttention.bias_k\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
Nonerù
bias_v5torch.nn.modules.activation.MultiheadAttention.bias_v\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
NonerN
	embed_dim8torch.nn.modules.activation.MultiheadAttention.embed_dim
AnyrD
kdim3torch.nn.modules.activation.MultiheadAttention.kdim
AnyrD
vdim3torch.nn.modules.activation.MultiheadAttention.vdim
Anyrb
_qkv_same_embed_dimBtorch.nn.modules.activation.MultiheadAttention._qkv_same_embed_dim
AnyrN
	num_heads8torch.nn.modules.activation.MultiheadAttention.num_heads
AnyrJ
dropout6torch.nn.modules.activation.MultiheadAttention.dropout
AnyrR
batch_first:torch.nn.modules.activation.MultiheadAttention.batch_first
AnyrL
head_dim7torch.nn.modules.activation.MultiheadAttention.head_dim
Anyrã
q_proj_weight<torch.nn.modules.activation.MultiheadAttention.q_proj_weight<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameterrã
k_proj_weight<torch.nn.modules.activation.MultiheadAttention.k_proj_weight<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameterrã
v_proj_weight<torch.nn.modules.activation.MultiheadAttention.v_proj_weight<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameterrç
in_proj_weight=torch.nn.modules.activation.MultiheadAttention.in_proj_weight<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameterrâ
in_proj_bias;torch.nn.modules.activation.MultiheadAttention.in_proj_bias<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameterr∑
out_proj7torch.nn.modules.activation.MultiheadAttention.out_projr
7torch.nn.modules.linear.NonDynamicallyQuantizableLinear"7torch.nn.modules.linear.NonDynamicallyQuantizableLinearrV
add_zero_attn<torch.nn.modules.activation.MultiheadAttention.add_zero_attn
Anyí

PReLU!torch.nn.modules.activation.PReLU"torch.nn.modules.module.Module*°
__init__*torch.nn.modules.activation.PReLU.__init__"
None*P
selfF
!torch.nn.modules.activation.PReLU"!torch.nn.modules.activation.PReLU*2
num_parameters
builtins.int"builtins.int *,
init 
builtins.float"builtins.float *
device
Any *
dtype
Any *P
reset_parameters2torch.nn.modules.activation.PReLU.reset_parameters*
self*Ì
forward)torch.nn.modules.activation.PReLU.forward",
torch._tensor.Tensor"torch._tensor.Tensor*P
selfF
!torch.nn.modules.activation.PReLU"!torch.nn.modules.activation.PReLU*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*™

extra_repr,torch.nn.modules.activation.PReLU.extra_repr"
builtins.str"builtins.str*P
selfF
!torch.nn.modules.activation.PReLU"!torch.nn.modules.activation.PReLUrå
__constants__/torch.nn.modules.activation.PReLU.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listr`
num_parameters0torch.nn.modules.activation.PReLU.num_parameters
builtins.int"builtins.intrP
init&torch.nn.modules.activation.PReLU.init 
builtins.float"builtins.floatrp
weight(torch.nn.modules.activation.PReLU.weight<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameter…
Softsign$torch.nn.modules.activation.Softsign"torch.nn.modules.module.Module*ˆ
forward,torch.nn.modules.activation.Softsign.forward",
torch._tensor.Tensor"torch._tensor.Tensor*V
selfL
$torch.nn.modules.activation.Softsign"$torch.nn.modules.activation.Softsign*7
input,
torch._tensor.Tensor"torch._tensor.Tensor”

Tanhshrink&torch.nn.modules.activation.Tanhshrink"torch.nn.modules.module.Module*¸
forward.torch.nn.modules.activation.Tanhshrink.forward",
torch._tensor.Tensor"torch._tensor.Tensor*Z
selfP
&torch.nn.modules.activation.Tanhshrink"&torch.nn.modules.activation.Tanhshrink*7
input,
torch._tensor.Tensor"torch._tensor.Tensor÷
Softmin#torch.nn.modules.activation.Softmin"torch.nn.modules.module.Module*È
__init__,torch.nn.modules.activation.Softmin.__init__"
None*T
selfJ
#torch.nn.modules.activation.Softmin"#torch.nn.modules.activation.Softmin*O
dimD
Union[builtins.int,None]
builtins.int"builtins.int
None *U
__setstate__0torch.nn.modules.activation.Softmin.__setstate__*
self*	
state*Û
forward+torch.nn.modules.activation.Softmin.forward",
torch._tensor.Tensor"torch._tensor.Tensor*T
selfJ
#torch.nn.modules.activation.Softmin"#torch.nn.modules.activation.Softmin*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*F

extra_repr.torch.nn.modules.activation.Softmin.extra_repr*
selfré
__constants__1torch.nn.modules.activation.Softmin.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrt
dim'torch.nn.modules.activation.Softmin.dimD
Union[builtins.int,None]
builtins.int"builtins.int
None¡
Softmax#torch.nn.modules.activation.Softmax"torch.nn.modules.module.Module*È
__init__,torch.nn.modules.activation.Softmax.__init__"
None*T
selfJ
#torch.nn.modules.activation.Softmax"#torch.nn.modules.activation.Softmax*O
dimD
Union[builtins.int,None]
builtins.int"builtins.int
None *U
__setstate__0torch.nn.modules.activation.Softmax.__setstate__*
self*	
state*Û
forward+torch.nn.modules.activation.Softmax.forward",
torch._tensor.Tensor"torch._tensor.Tensor*T
selfJ
#torch.nn.modules.activation.Softmax"#torch.nn.modules.activation.Softmax*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*∞

extra_repr.torch.nn.modules.activation.Softmax.extra_repr"
builtins.str"builtins.str*T
selfJ
#torch.nn.modules.activation.Softmax"#torch.nn.modules.activation.Softmaxré
__constants__1torch.nn.modules.activation.Softmax.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrt
dim'torch.nn.modules.activation.Softmax.dimD
Union[builtins.int,None]
builtins.int"builtins.int
NoneŒ
	Softmax2d%torch.nn.modules.activation.Softmax2d"torch.nn.modules.module.Module*˘
forward-torch.nn.modules.activation.Softmax2d.forward",
torch._tensor.Tensor"torch._tensor.Tensor*X
selfN
%torch.nn.modules.activation.Softmax2d"%torch.nn.modules.activation.Softmax2d*7
input,
torch._tensor.Tensor"torch._tensor.Tensor˙

LogSoftmax&torch.nn.modules.activation.LogSoftmax"torch.nn.modules.module.Module*Ú
__init__/torch.nn.modules.activation.LogSoftmax.__init__"
None*Z
selfP
&torch.nn.modules.activation.LogSoftmax"&torch.nn.modules.activation.LogSoftmax*O
dimD
Union[builtins.int,None]
builtins.int"builtins.int
None *X
__setstate__3torch.nn.modules.activation.LogSoftmax.__setstate__*
self*	
state*¸
forward.torch.nn.modules.activation.LogSoftmax.forward",
torch._tensor.Tensor"torch._tensor.Tensor*Z
selfP
&torch.nn.modules.activation.LogSoftmax"&torch.nn.modules.activation.LogSoftmax*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*I

extra_repr1torch.nn.modules.activation.LogSoftmax.extra_repr*
selfrë
__constants__4torch.nn.modules.activation.LogSoftmax.__constants__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listrw
dim*torch.nn.modules.activation.LogSoftmax.dimD
Union[builtins.int,None]
builtins.int"builtins.int
None«
_check_arg_device-torch.nn.modules.activation._check_arg_device"
builtins.bool"builtins.bool*c
x\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None…
_arg_requires_grad.torch.nn.modules.activation._arg_requires_grad"
builtins.bool"builtins.bool*c
x\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
NoneF
_is_make_fx_tracing/torch.nn.modules.activation._is_make_fx_tracing*ó
__annotations__+torch.nn.modules.activation.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*
Ftorch.nn.functional *z
__all__#torch.nn.modules.activation.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list