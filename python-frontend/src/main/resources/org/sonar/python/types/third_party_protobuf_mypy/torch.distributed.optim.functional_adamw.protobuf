
(torch.distributed.optim.functional_adamw½
_FunctionalAdamW9torch.distributed.optim.functional_adamw._FunctionalAdamW"builtins.object*Ï
__init__Btorch.distributed.optim.functional_adamw._FunctionalAdamW.__init__"
None*€
selfv
9torch.distributed.optim.functional_adamw._FunctionalAdamW"9torch.distributed.optim.functional_adamw._FunctionalAdamW*n
paramsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list**
lr 
builtins.float"builtins.float *y
betasl
$Tuple[builtins.float,builtins.float] 
builtins.float"builtins.float 
builtins.float"builtins.float *+
eps 
builtins.float"builtins.float *4
weight_decay 
builtins.float"builtins.float *-
amsgrad
builtins.bool"builtins.bool *.
maximize
builtins.bool"builtins.bool *-
foreach
builtins.bool"builtins.bool *+
fused
builtins.bool"builtins.bool *=
_allow_empty_param_list
builtins.bool"builtins.bool *ÿ

step_paramDtorch.distributed.optim.functional_adamw._FunctionalAdamW.step_param"
Any*€
selfv
9torch.distributed.optim.functional_adamw._FunctionalAdamW"9torch.distributed.optim.functional_adamw._FunctionalAdamW*7
param,
torch._tensor.Tensor"torch._tensor.Tensor*f
grad\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None*ƒ
step>torch.distributed.optim.functional_adamw._FunctionalAdamW.step"
Any*€
selfv
9torch.distributed.optim.functional_adamw._FunctionalAdamW"9torch.distributed.optim.functional_adamw._FunctionalAdamW*®
	gradientsž
/builtins.list[Union[torch._tensor.Tensor,None]]\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None"builtins.list8rË
defaultsBtorch.distributed.optim.functional_adamw._FunctionalAdamW.defaults{
*builtins.dict[builtins.str,builtins.float]
builtins.str"builtins.str 
builtins.float"builtins.float"builtins.dictrl
amsgradAtorch.distributed.optim.functional_adamw._FunctionalAdamW.amsgrad
builtins.bool"builtins.boolrn
maximizeBtorch.distributed.optim.functional_adamw._FunctionalAdamW.maximize
builtins.bool"builtins.boolrl
foreachAtorch.distributed.optim.functional_adamw._FunctionalAdamW.foreach
builtins.bool"builtins.boolrh
fused?torch.distributed.optim.functional_adamw._FunctionalAdamW.fused
builtins.bool"builtins.boolrQ
state?torch.distributed.optim.functional_adamw._FunctionalAdamW.state
Anyr©
param_groupEtorch.distributed.optim.functional_adamw._FunctionalAdamW.param_groupÒ
?builtins.dict[builtins.str,builtins.list[torch._tensor.Tensor]]
builtins.str"builtins.strb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list"builtins.dict*¤
__annotations__8torch.distributed.optim.functional_adamw.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*
Ftorch.optim._functional *‡
__all__0torch.distributed.optim.functional_adamw.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list