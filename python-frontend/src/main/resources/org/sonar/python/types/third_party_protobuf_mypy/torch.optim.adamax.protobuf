
torch.optim.adamax‰
Adamaxtorch.optim.adamax.Adamax"torch.optim.optimizer.Optimizer*ú
__init__"torch.optim.adamax.Adamax.__init__"
None*@
self6
torch.optim.adamax.Adamax"torch.optim.adamax.Adamax*ƒ
paramsö
hTypeAlias[Union[typing.Iterable[torch._tensor.Tensor],typing.Iterable[builtins.dict[builtins.str,Any]]]]è
]Union[typing.Iterable[torch._tensor.Tensor],typing.Iterable[builtins.dict[builtins.str,Any]]]f
%typing.Iterable[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Iterableœ
0typing.Iterable[builtins.dict[builtins.str,Any]]W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict"typing.Iterable"torch.optim.optimizer.ParamsT**
lr 
builtins.float"builtins.float *y
betasl
$Tuple[builtins.float,builtins.float] 
builtins.float"builtins.float 
builtins.float"builtins.float *+
eps 
builtins.float"builtins.float *4
weight_decay 
builtins.float"builtins.float *V
foreachG
Union[builtins.bool,None]
builtins.bool"builtins.bool
None *.
maximize
builtins.bool"builtins.bool *4
differentiable
builtins.bool"builtins.bool *0

capturable
builtins.bool"builtins.bool *K
__setstate__&torch.optim.adamax.Adamax.__setstate__*
self*	
state*—
_init_group%torch.optim.adamax.Adamax._init_group*
self*	
group*
params_with_grad*	
grads*
exp_avgs*
exp_infs*
state_steps*_
steptorch.optim.adamax.Adamax.step*
self*
closure 0:_use_grad_for_differentiable¨
_single_tensor_adamax(torch.optim.adamax._single_tensor_adamax"
Any*n
paramsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list*m
gradsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list*p
exp_avgsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list*p
exp_infsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list*s
state_stepsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list*)
eps 
builtins.float"builtins.float*+
beta1 
builtins.float"builtins.float*+
beta2 
builtins.float"builtins.float*(
lr 
builtins.float"builtins.float*2
weight_decay 
builtins.float"builtins.float*,
maximize
builtins.bool"builtins.bool*2
differentiable
builtins.bool"builtins.bool*.

capturable
builtins.bool"builtins.bool*/
has_complex
builtins.bool"builtins.bool¦
_multi_tensor_adamax'torch.optim.adamax._multi_tensor_adamax"
Any*n
paramsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list*m
gradsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list*p
exp_avgsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list*p
exp_infsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list*s
state_stepsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list*)
eps 
builtins.float"builtins.float*+
beta1 
builtins.float"builtins.float*+
beta2 
builtins.float"builtins.float*(
lr 
builtins.float"builtins.float*2
weight_decay 
builtins.float"builtins.float*,
maximize
builtins.bool"builtins.bool*2
differentiable
builtins.bool"builtins.bool*.

capturable
builtins.bool"builtins.bool*/
has_complex
builtins.bool"builtins.bool*Ž
__annotations__"torch.optim.adamax.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*q
__all__torch.optim.adamax.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list