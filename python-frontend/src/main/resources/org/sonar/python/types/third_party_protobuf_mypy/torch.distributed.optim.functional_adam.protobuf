
'torch.distributed.optim.functional_adam—
_FunctionalAdam7torch.distributed.optim.functional_adam._FunctionalAdam"builtins.object*È
__init__@torch.distributed.optim.functional_adam._FunctionalAdam.__init__"
None*|
selfr
7torch.distributed.optim.functional_adam._FunctionalAdam"7torch.distributed.optim.functional_adam._FunctionalAdam*n
paramsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list**
lr 
builtins.float"builtins.float *y
betasl
$Tuple[builtins.float,builtins.float] 
builtins.float"builtins.float 
builtins.float"builtins.float *+
eps 
builtins.float"builtins.float *4
weight_decay 
builtins.float"builtins.float *-
amsgrad
builtins.bool"builtins.bool *.
maximize
builtins.bool"builtins.bool *-
foreach
builtins.bool"builtins.bool *+
fused
builtins.bool"builtins.bool *=
_allow_empty_param_list
builtins.bool"builtins.bool *ø

step_paramBtorch.distributed.optim.functional_adam._FunctionalAdam.step_param"
Any*|
selfr
7torch.distributed.optim.functional_adam._FunctionalAdam"7torch.distributed.optim.functional_adam._FunctionalAdam*7
param,
torch._tensor.Tensor"torch._tensor.Tensor*f
grad\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None*ü
step<torch.distributed.optim.functional_adam._FunctionalAdam.step"
Any*|
selfr
7torch.distributed.optim.functional_adam._FunctionalAdam"7torch.distributed.optim.functional_adam._FunctionalAdam*®
	gradientsž
/builtins.list[Union[torch._tensor.Tensor,None]]\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None"builtins.list8rÉ
defaults@torch.distributed.optim.functional_adam._FunctionalAdam.defaults{
*builtins.dict[builtins.str,builtins.float]
builtins.str"builtins.str 
builtins.float"builtins.float"builtins.dictrj
amsgrad?torch.distributed.optim.functional_adam._FunctionalAdam.amsgrad
builtins.bool"builtins.boolrl
maximize@torch.distributed.optim.functional_adam._FunctionalAdam.maximize
builtins.bool"builtins.boolrj
foreach?torch.distributed.optim.functional_adam._FunctionalAdam.foreach
builtins.bool"builtins.boolrf
fused=torch.distributed.optim.functional_adam._FunctionalAdam.fused
builtins.bool"builtins.boolrO
state=torch.distributed.optim.functional_adam._FunctionalAdam.state
Anyr§
param_groupCtorch.distributed.optim.functional_adam._FunctionalAdam.param_groupÒ
?builtins.dict[builtins.str,builtins.list[torch._tensor.Tensor]]
builtins.str"builtins.strb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list"builtins.dict*£
__annotations__7torch.distributed.optim.functional_adam.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*
Ftorch.optim._functional *†
__all__/torch.distributed.optim.functional_adam.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list