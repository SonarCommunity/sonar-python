
)torch.distributed.optim.functional_adamax 
_FunctionalAdamax;torch.distributed.optim.functional_adamax._FunctionalAdamax"builtins.object*ù
__init__Dtorch.distributed.optim.functional_adamax._FunctionalAdamax.__init__"
None*„
selfz
;torch.distributed.optim.functional_adamax._FunctionalAdamax";torch.distributed.optim.functional_adamax._FunctionalAdamax*n
paramsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list**
lr 
builtins.float"builtins.float *y
betasl
$Tuple[builtins.float,builtins.float] 
builtins.float"builtins.float 
builtins.float"builtins.float *+
eps 
builtins.float"builtins.float *4
weight_decay 
builtins.float"builtins.float *-
foreach
builtins.bool"builtins.bool *.
maximize
builtins.bool"builtins.bool *=
_allow_empty_param_list
builtins.bool"builtins.bool *‰
step@torch.distributed.optim.functional_adamax._FunctionalAdamax.step"
Any*„
selfz
;torch.distributed.optim.functional_adamax._FunctionalAdamax";torch.distributed.optim.functional_adamax._FunctionalAdamax*®
	gradientsž
/builtins.list[Union[torch._tensor.Tensor,None]]\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None"builtins.list8rÍ
defaultsDtorch.distributed.optim.functional_adamax._FunctionalAdamax.defaults{
*builtins.dict[builtins.str,builtins.float]
builtins.str"builtins.str 
builtins.float"builtins.float"builtins.dictrn
foreachCtorch.distributed.optim.functional_adamax._FunctionalAdamax.foreach
builtins.bool"builtins.boolrp
maximizeDtorch.distributed.optim.functional_adamax._FunctionalAdamax.maximize
builtins.bool"builtins.boolrS
stateAtorch.distributed.optim.functional_adamax._FunctionalAdamax.state
Anyr«
param_groupGtorch.distributed.optim.functional_adamax._FunctionalAdamax.param_groupÒ
?builtins.dict[builtins.str,builtins.list[torch._tensor.Tensor]]
builtins.str"builtins.strb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list"builtins.dict*¥
__annotations__9torch.distributed.optim.functional_adamax.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*
Ftorch.optim._functional *ˆ
__all__1torch.distributed.optim.functional_adamax.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list